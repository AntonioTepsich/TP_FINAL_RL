# ============================================================================
# PHASE 1: QUICK SEARCH - BÃºsqueda RÃ¡pida de HiperparÃ¡metros CrÃ­ticos
# ============================================================================
# Objetivo: Identificar mejoras obvias en parÃ¡metros de alto impacto
# DuraciÃ³n estimada: 6 trials Ã— 30 min = ~3 horas
# Baseline: lr=3e-4, hidden=256, ent=0.02 (tu configuraciÃ³n actual)
#
# Estrategia: Random search enfocado en los 3 parÃ¡metros MÃS importantes:
#   1. Learning Rate (lr_start) - Impacto CRÃTICO
#   2. Hidden Size - Capacidad de la red
#   3. Entropy (ent_start) - Balance exploraciÃ³n/explotaciÃ³n
#
# Uso:
#   python run_experiment.py --config config_template.yaml \
#       --search --search-config phase1_quick_search.yaml
# ============================================================================

strategy: random
n_trials: 6
seed: 42

search_space:
  # ==========================================================================
  # 1. LEARNING RATE - PARÃMETRO MÃS CRÃTICO ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
  # ==========================================================================
  # Controla la velocidad de aprendizaje
  # - Muy alto (>1e-3): Puede diverger o ser inestable
  # - Muy bajo (<1e-4): Aprende muy lento, puede quedar atascado
  #
  # Tu baseline: 3e-4 (estÃ¡ndar para PPO)
  # ExploraciÃ³n: Probar valores arriba y abajo

  lr_start:
    - 0.0001   # 1e-4  - Conservador (aprendizaje mÃ¡s lento pero estable)
    - 0.0003   # 3e-4  - TU BASELINE âœ“ (estÃ¡ndar PPO)
    - 0.0005   # 5e-4  - Agresivo (aprendizaje mÃ¡s rÃ¡pido)
    - 0.001    # 1e-3  - Muy agresivo (puede ser inestable pero a veces mejor)

  # ==========================================================================
  # 2. HIDDEN SIZE - CAPACIDAD DEL MODELO ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
  # ==========================================================================
  # TamaÃ±o de las capas ocultas de la red neuronal
  # - MÃ¡s pequeÃ±o (128): MÃ¡s rÃ¡pido, menos expresivo
  # - MÃ¡s grande (512): MÃ¡s expresivo, mÃ¡s lento, puede overfittear
  #
  # Tu baseline: 256 (balance razonable)
  # ExploraciÃ³n: Probar mÃ¡s pequeÃ±o y mÃ¡s grande

  hidden_size:
    - 128      # PequeÃ±o - MÃ¡s rÃ¡pido (~20% menos tiempo)
    - 256      # TU BASELINE âœ“ - Balance estÃ¡ndar
    - 512      # Grande - MÃ¡s capacidad (Flappy Bird podrÃ­a beneficiarse)

  # ==========================================================================
  # 3. ENTROPY - EXPLORACIÃ“N VS EXPLOTACIÃ“N ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
  # ==========================================================================
  # Coeficiente de entropÃ­a inicial (controla exploraciÃ³n)
  # - Bajo (0.01): Menos exploraciÃ³n, explotaciÃ³n mÃ¡s temprana
  # - Alto (0.03-0.05): MÃ¡s exploraciÃ³n, evita mÃ­nimos locales
  #
  # Tu baseline: 0.02 (moderado)
  # ExploraciÃ³n: Probar mÃ¡s y menos exploraciÃ³n

  ent_start:
    - 0.01     # Baja exploraciÃ³n - Converge mÃ¡s rÃ¡pido
    - 0.02     # TU BASELINE âœ“ - Moderado
    - 0.03     # Alta exploraciÃ³n - Explora mÃ¡s al inicio
    - 0.05     # Muy alta exploraciÃ³n - Arriesgado pero puede encontrar mejores polÃ­ticas

  # ==========================================================================
  # 4. LR SCHEDULE - BONUS (bajo impacto pero fÃ¡cil de probar)
  # ==========================================================================
  # CÃ³mo decae el learning rate durante el entrenamiento
  # - cosine: Decaimiento suave (mejor para entrenamientos largos)
  # - linear: Decaimiento lineal (mÃ¡s predecible)
  #
  # Tu baseline: cosine
  # ExploraciÃ³n: Probar linear para comparar

  lr_schedule:
    - cosine   # TU BASELINE âœ“ - Suave, recomendado
    - linear   # Alternativa comÃºn

# ============================================================================
# NOTAS IMPORTANTES
# ============================================================================
#
# 1. PARÃMETROS QUE *NO* ESTAMOS TOCANDO (y por quÃ©):
#    - gamma (0.99): Valor estÃ¡ndar casi universal en RL
#    - lambda_gae (0.95): Valor estÃ¡ndar, menos impacto que LR
#    - clip_epsilon (0.2): EstÃ¡ndar PPO, cambiar tiene poco impacto
#    - vf_coef (0.5): EstÃ¡ndar, solo afinar en fase 2 si es necesario
#    - n_envs (16): Suficiente paralelizaciÃ³n, aumentar solo si sobran recursos
#
# 2. COMBINACIONES POSIBLES:
#    - 4 lr_start Ã— 3 hidden_size Ã— 4 ent_start Ã— 2 lr_schedule = 96 combinaciones
#    - Random search probarÃ¡ 6 de estas 96 (mÃ¡s eficiente que grid)
#
# 3. QUÃ‰ ESPERAR:
#    - Baseline esperado: ~40 tubos (tu config actual)
#    - Meta: Encontrar algo que llegue a 45-50+ tubos
#    - Si encuentras 55+ tubos: Â¡Jackpot! ğŸ‰
#
# 4. CÃ“MO INTERPRETAR RESULTADOS:
#    - Mira "best_score" (tubos pasados), no solo reward
#    - Mira tambiÃ©n la estabilidad (final_mean_score vs best_score)
#    - Si lr muy alto causa crash â†’ excluir en Fase 2
#    - Si hidden=512 es mucho mejor â†’ usar ese en Fase 2
#
# 5. DESPUÃ‰S DE LA FASE 1:
#    Ver resultados con:
#      python view_results.py --bucket ppo-flappy-bird --best 6
#
#    Identificar patrones:
#      - Â¿QuÃ© lr_start funcionÃ³ mejor?
#      - Â¿Hidden size mÃ¡s grande ayuda?
#      - Â¿MÃ¡s exploraciÃ³n (ent_start) dio mejores resultados?
#
# 6. PRÃ“XIMOS PASOS:
#    - Si encuentras mejoras claras â†’ Crear phase2_focused_search.yaml
#    - Si no hay mejoras â†’ Considerar que tu baseline ya es bueno
#    - Si hay crash/divergencia â†’ Ajustar rangos en Fase 2
#
# ============================================================================
# EJEMPLO DE EJECUCIÃ“N
# ============================================================================
#
# En Colab o terminal:
#
# # 1. Ejecutar bÃºsqueda
# !python run_experiment.py \
#     --config config_template.yaml \
#     --search \
#     --search-config phase1_quick_search.yaml
#
# # 2. Monitorear progreso (ejecutar en celda aparte)
# !python view_results.py --bucket ppo-flappy-bird --compare --top 6
#
# # 3. Ver mejores resultados al terminar
# !python view_results.py --bucket ppo-flappy-bird --best 6
#
# ============================================================================
