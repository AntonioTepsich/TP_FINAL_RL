# üî¨ Plan de Diagn√≥stico y Mejora - Flappy Bird PPO

## üìã Checklist Priorizada

### ‚úÖ Scripts Creados

| Script | Tiempo | Prop√≥sito |
|--------|--------|-----------|
| `check_obs.py` | 30s | Verificar si son vectores o p√≠xeles |
| `check_gae.py` | 5s | Test matem√°tico de GAE |
| `test_rapido.py` | 1-2min | Diagn√≥stico r√°pido con m√©tricas PPO |
| `train_improved.py` | 15-20min | Entrenamiento completo con todas las fixes |

---

## üöÄ Plan de Acci√≥n (Orden Sugerido)

### **PASO 1: Verificar Observaciones** (30 segundos)
```bash
python check_obs.py
```

**Qu√© buscar:**
- ‚úÖ Si son **vectores** (12 features): Perfecto, contin√∫a al Paso 2
- ‚ùå Si son **p√≠xeles**: Necesitas usar CNN (en `ppo.py`) o wrapper para extraer features

**Acci√≥n si son p√≠xeles:**
- Opci√≥n A: Usa el env con render_mode diferente para obtener features
- Opci√≥n B: Crea wrapper que extrae (posici√≥n bird, velocidad, distancia a pipes)

---

### **PASO 2: Verificar GAE** (5 segundos)
```bash
python check_gae.py
```

**Qu√© buscar:**
- ‚úÖ Todos los checks pasan
- ‚ùå Si falla alg√∫n check: revisar implementaci√≥n de GAE

---

### **PASO 3: Test R√°pido con Diagn√≥sticos** (1-2 minutos)
```bash
python test_rapido.py
```

**Qu√© observar en los logs:**

#### üìä **POLICY Metrics:**
| M√©trica | Rango Bueno | Acci√≥n si Fuera de Rango |
|---------|-------------|--------------------------|
| **Ratio** | 0.8 - 1.2 | <0.8 o >1.3: Baj√° LR |
| **KL Div** | < 0.02 | >0.03: Baj√° LR o epochs |
| **Clip Frac** | 0.1 - 0.4 | <0.05: Sub√≠ LR / >0.5: Baj√° LR |

#### üìà **VALUE Metrics:**
| M√©trica | Rango Bueno | Acci√≥n si Fuera de Rango |
|---------|-------------|--------------------------|
| **Explained Var** | > 0.3 | <0.2: Sub√≠ `vf_coef` o capacidad red |

#### üé≤ **EXPLORATION Metrics:**
| M√©trica | Rango Bueno | Acci√≥n si Fuera de Rango |
|---------|-------------|--------------------------|
| **Entropy** | 0.05 - 0.3 | <0.01: Sub√≠ `ent_coef` |

---

### **PASO 4: Ajustar Hiperpar√°metros**

Basado en los diagn√≥sticos del Paso 3, edit√° `train_improved.py`:

```python
# CONFIGURACI√ìN (l√≠neas 132-140)
N_ENVS = 16          # 8-24 seg√∫n tu CPU/GPU
T = 256              # 128-512 (m√°s = mejor GAE, menos = updates m√°s frecuentes)

LR_START = 3e-4      # Baj√° a 1e-4 si ratio/KL muy alto
LR_END = 1e-5
ENT_START = 0.02     # Sub√≠ a 0.03-0.05 si entrop√≠a cae muy r√°pido
ENT_END = 0.005

# En agent (l√≠nea 154)
clip_eps=0.2,        # Standard
ent_coef=ENT_START,  
vf_coef=0.5,         # Sub√≠ a 1.0 si explained_var < 0.2
```

---

### **PASO 5: Entrenamiento Completo** (15-20 minutos)
```bash
python train_improved.py
```

**Monitorear durante el entrenamiento:**

1. **Primeros 100k steps:**
   - Reward deber√≠a pasar de ~0 a 10-20
   - Entropy bajando de 0.02 ‚Üí 0.015
   - Explained Var subiendo a >0.5

2. **500k steps:**
   - Reward ~50-100
   - Entropy ~0.01
   - KL estable <0.02

3. **1M steps:**
   - Reward >100-200
   - Pol√≠tica estable

---

## üîß Troubleshooting Com√∫n

### Problema 1: No aprende (reward estancado)
**S√≠ntomas:** Reward se queda en 0-5 por mucho tiempo

**Checks:**
```python
# En train_improved.py, agreg√° despu√©s del rollout:
print(f"Sample rewards: {rews_t[:,0][:10]}")  # Ver si hay rewards positivos
print(f"Sample actions: {acts_t[:50]}")        # Ver distribuci√≥n de acciones
```

**Soluciones:**
- Si >90% acciones son la misma: Sub√≠ `ent_coef` a 0.03-0.05
- Si rewards todos negativos: Verific√° que el env da +1 por sobrevivir
- Si explained_var <0: Red no aprende, aument√° capacidad (hidden=256‚Üí512)

---

### Problema 2: Inestable (reward sube y baja mucho)
**S√≠ntomas:** Reward llega a 50, luego cae a 10, sube a 80, etc.

**Checks:**
- Mir√° KL div y ratio en logs
- Si KL >0.03 o ratio >1.5: Updates muy agresivos

**Soluciones:**
1. Baj√° LR: `3e-4 ‚Üí 1e-4`
2. Reduc√≠ epochs: `4 ‚Üí 3`
3. Aument√° `N_ENVS` para m√°s estabilidad

---

### Problema 3: Aprende pero no llega lejos
**S√≠ntomas:** Reward estable en 30-50 pero no sube m√°s

**Checks:**
- Entropy muy baja (<0.005): Pol√≠tica muy determinista, no explora
- Clip frac muy alto (>0.6): Updates muy conservadores

**Soluciones:**
1. Si entropy baja: Ajust√° schedule para que baje m√°s lento
2. Aument√° `T` (256‚Üí512) para mejor estimaci√≥n GAE
3. Prob√° diferentes seeds

---

## üìä Mejoras Aplicadas vs Original

| Aspecto | Original (`train_vector.py`) | Mejorado (`train_improved.py`) |
|---------|------------------------------|--------------------------------|
| **Normalizaci√≥n** | ‚ùå No | ‚úÖ Online normalization |
| **M√©tricas PPO** | ‚ùå Solo loss | ‚úÖ KL, ratio, clip_frac, EV |
| **Schedules** | ‚ùå Fijo | ‚úÖ LR cosine, entropy linear |
| **Diagn√≥sticos** | ‚ùå B√°sico | ‚úÖ Warnings autom√°ticos |
| **Value clipping** | ‚ùå No | ‚úÖ S√≠ |
| **Logging** | ‚ö†Ô∏è B√°sico | ‚úÖ Completo con interpretaci√≥n |

---

## üéØ Resultados Esperados

### Con observaciones vectoriales (12 features):
- **100k steps** (~2 min): Reward ~10-30
- **500k steps** (~8 min): Reward ~50-100
- **1M steps** (~15 min): Reward >100-200

### Se√±ales de √©xito:
- ‚úÖ Explained Variance >0.5 en primeros 200k steps
- ‚úÖ KL div estable <0.02
- ‚úÖ Entropy baja gradualmente (no colapsa en 0)
- ‚úÖ Reward crece monot√≥nicamente (con ruido)

---

## üî¨ Experimentos Adicionales (Opcional)

### A) Probar diferentes T (rollout length)
```bash
# Edit√° en train_improved.py l√≠nea 134
T = 128   # M√°s updates, se√±ales frescas
T = 512   # Mejor GAE, m√°s estable
```

### B) Probar diferentes N_ENVS
```bash
N_ENVS = 8    # M√°s r√°pido, menos estable
N_ENVS = 24   # M√°s lento, m√°s estable
```

### C) Comparar con/sin normalizaci√≥n
```bash
# En train_improved.py l√≠nea 149
vec = VecEnv(n_envs=N_ENVS, normalize=False)  # Sin normalizaci√≥n
```

---

## üìù Notas Importantes

1. **Reproducibilidad**: Seeds fijas en `reset(seed=i)` (ya implementado)

2. **Guardado de modelos**:
   - `best_model_improved.pt`: Mejor modelo por reward
   - `checkpoint_improved_250000.pt`: Checkpoints cada 250k

3. **Cargar modelo guardado**:
```python
model = VectorActorCritic(obs_dim=12, n_actions=2)
model.load_state_dict(torch.load('best_model_improved.pt'))
model.eval()
```

4. **Si vas a p√≠xeles**:
   - Necesit√°s CNN (ya en `ppo.py`)
   - Cambi√° obs/255.0
   - Esper√° 2-3 horas en vez de 15 min
   - Ajust√° `ConvEncoder.out_dim` seg√∫n resoluci√≥n

---

## ‚úÖ Checklist Final Antes de 1M Steps

- [ ] `check_obs.py` confirma vectores 12D
- [ ] `check_gae.py` pasa todos los tests
- [ ] `test_rapido.py` muestra m√©tricas razonables
- [ ] Explained Var >0.3 en test r√°pido
- [ ] KL <0.03 en test r√°pido
- [ ] Ajustaste hiperpar√°metros seg√∫n diagn√≥sticos
- [ ] GPU/CPU libre para 15-20 min

**Si todos ‚úÖ, corre:**
```bash
python train_improved.py
```

---

## üÜò Si Nada Funciona

1. Compart√≠ output de `check_obs.py`
2. Compart√≠ primeros logs de `test_rapido.py` (m√©tricas PPO)
3. Compart√≠ gr√°fica de reward vs steps (aunque sea solo valores)

Podemos debugear desde ah√≠ con info concreta.
