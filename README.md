# TP Final Reinforcement Learning - PPO-Clip en Flappy Bird

**Autores:** Michelle Chloe Berezovsky y Antonio Santiago Tepsich
**Fecha:** 2025  
**InstituciÃ³n:** Universidad de San AndrÃ©s

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un agente de Reinforcement Learning utilizando el algoritmo **PPO-Clip (Proximal Policy Optimization)** para resolver el juego **Flappy Bird**. El agente aprende a jugar de manera autÃ³noma mediante interacciÃ³n con el entorno y optimizaciÃ³n de polÃ­ticas.

### CaracterÃ­sticas Principales
- **Algoritmo:** PPO-Clip con entropÃ­a adaptativa
- **Entorno:** Flappy Bird (Gymnasium)
- **Red Neuronal:** Actor-Critic con capas compartidas
- **NormalizaciÃ³n:** Observaciones normalizadas online
- **Logging:** TensorBoard para monitoreo de entrenamiento
- **VectorizaciÃ³n:** Entrenamiento paralelo con mÃºltiples entornos

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
TP_FINAL_RL/
â”œâ”€â”€ ppo.py                      # ImplementaciÃ³n del algoritmo PPO-Clip
â”œâ”€â”€ train_vector_improved.py    # Script principal de entrenamiento
â”œâ”€â”€ watch_play.py              # VisualizaciÃ³n del agente entrenado
â”œâ”€â”€ tensorboard_logger.py      # Sistema de logging para TensorBoard
â”œâ”€â”€ requirements.txt           # Dependencias del proyecto
â”œâ”€â”€ best_model_improved.pt     # Modelo entrenado (checkpoint)
â”œâ”€â”€ checks/                    # DiagnÃ³sticos y verificaciones
â”‚   â”œâ”€â”€ check_obs.py
â”‚   â””â”€â”€ DIAGNOSTICOS_README.md
â””â”€â”€ runs/                      # Logs de TensorBoard
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/AntonioTepsich/TP_FINAL_RL.git
cd TP_FINAL_RL
```

### 2. Instalar Dependencias
```bash
pip install -r requirements.txt
```

**Dependencias principales:**
- PyTorch >= 2.0.0
- Gymnasium >= 0.29.0
- Flappy Bird Gymnasium >= 0.3.0
- TensorBoard >= 2.14.0
- NumPy >= 1.24.0

---

## ğŸ® Uso

### Entrenar un Nuevo Modelo

Para entrenar un agente desde cero:

```bash
python train_vector_improved.py
```

**ConfiguraciÃ³n de entrenamiento:**
- **Entornos paralelos:** 8
- **Pasos por iteraciÃ³n:** 2048
- **Ã‰pocas por actualizaciÃ³n:** 4
- **Learning rate inicial:** 3e-4
- **Clip epsilon:** 0.2
- **EntropÃ­a adaptativa:** SÃ­

El entrenamiento guardarÃ¡:
- Checkpoints en `best_model_improved.pt`
- Logs de TensorBoard en `runs/`

### Visualizar el Entrenamiento

Para monitorear el progreso en tiempo real:

```bash
tensorboard --logdir=runs
```

Luego abre tu navegador en `http://localhost:6006`

### Evaluar el Modelo Entrenado

Para ver al agente jugar:

```bash
python watch_play.py
```

**ParÃ¡metros configurables** (en el archivo):
- `episodes`: NÃºmero de episodios a visualizar (default: 5)
- `fps`: Velocidad de renderizado (default: 60)
- `debug`: Mostrar informaciÃ³n de debugging (default: True)

---

## ğŸ“Š Componentes Principales

### PPO-Clip (`ppo.py`)
ImplementaciÃ³n del algoritmo Proximal Policy Optimization con:
- Clipping de probabilidades para estabilidad
- Generalised Advantage Estimation (GAE)
- OptimizaciÃ³n de Actor y Critic simultÃ¡neos

### Red Neuronal Actor-Critic
Arquitectura de la red:
```
Input (180 observaciones) 
    â†“
Capa Compartida (256 unidades) â†’ ReLU
    â†“
Capa Compartida (256 unidades) â†’ ReLU
    â”œâ”€â†’ Policy Head (2 acciones)
    â””â”€â†’ Value Head (1 valor)
```

### NormalizaciÃ³n de Observaciones
Wrapper personalizado que mantiene estadÃ­sticas mÃ³viles (media y varianza) para normalizar observaciones durante el entrenamiento, mejorando la estabilidad del aprendizaje.

---

## ğŸ“ˆ Resultados

El modelo aprende progresivamente a:
1. **Evitar colisiones** con las tuberÃ­as
2. **Mantener altura Ã³ptima** en el juego
3. **Maximizar la recompensa** acumulada

Los mejores modelos logran superar mÃºltiples obstÃ¡culos consecutivamente.

---

## ğŸ› ï¸ DiagnÃ³sticos

La carpeta `checks/` contiene herramientas de diagnÃ³stico:
- `check_obs.py`: VerificaciÃ³n de dimensiones de observaciones
- `DIAGNOSTICOS_README.md`: GuÃ­a de troubleshooting