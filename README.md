# TP Final Reinforcement Learning - PPO-Clip en Flappy Bird

**Autores:** Michelle Chloe Berezovsky y Antonio Santiago Tepsich
**Fecha:** 2025  
**InstituciÃ³n:** Universidad de San AndrÃ©s

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un agente de Reinforcement Learning utilizando el algoritmo **PPO-Clip (Proximal Policy Optimization)** para resolver el juego **Flappy Bird**. El agente aprende a jugar de manera autÃ³noma mediante interacciÃ³n con el entorno y optimizaciÃ³n de polÃ­ticas.

### CaracterÃ­sticas Principales
- **Algoritmo:** PPO-Clip con entropÃ­a adaptativa
- **Entorno:** Flappy Bird (Gymnasium)
- **Red Neuronal:** Actor-Critic con capas compartidas
- **NormalizaciÃ³n:** Observaciones normalizadas online
- **Logging:** TensorBoard para monitoreo de entrenamiento
- **VectorizaciÃ³n:** Entrenamiento paralelo con mÃºltiples entornos

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
TP_FINAL_RL/
â”œâ”€â”€ ppo.py                      # ImplementaciÃ³n del algoritmo PPO-Clip
â”œâ”€â”€ train_vector_improved.py    # Script principal de entrenamiento
â”œâ”€â”€ run_experiment.py           # Script para ejecutar experimentos personalizados
â”œâ”€â”€ evaluate.py                 # Script para evaluar el agente entrenado
â”œâ”€â”€ jugar.py                    # Script alternativo para jugar con el agente
â”œâ”€â”€ watch_play.py               # VisualizaciÃ³n del agente entrenado
â”œâ”€â”€ requirements.txt            # Dependencias del proyecto
â”œâ”€â”€ tensorboard_logger.py       # Sistema de logging para TensorBoard
â”œâ”€â”€ tools/                      # Utilidades y helpers
â”‚   â”œâ”€â”€ gcs_manager.py
â”‚   â”œâ”€â”€ tensorboard_logger.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ probe_envs/                 # Scripts de prueba y comparaciÃ³n de entornos
â”‚   â”œâ”€â”€ train_ppo_custom_acrobot.py
â”‚   â”œâ”€â”€ train_ppo_custom_cartpole_baseline.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ focused_search_project/     # Configuraciones de bÃºsqueda y experimentos
â”‚   â”œâ”€â”€ config_baseline_v1.yaml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/                # Resultados de experimentos recientes
â”‚   â””â”€â”€ ...
â”œâ”€â”€ exp_old/                    # Resultados de experimentos antiguos
â”‚   â””â”€â”€ ...
â”œâ”€â”€ evaluation_results/         # Resultados de evaluaciones (CSV)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tensorboard/                # Carpeta de logs de TensorBoard
â”‚   â””â”€â”€ runs/
â””â”€â”€ __pycache__/                # Archivos temporales de Python
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/AntonioTepsich/TP_FINAL_RL.git
cd TP_FINAL_RL
```

### 2. Instalar Dependencias
```bash
pip install -r requirements.txt
```

**Dependencias principales:**
- PyTorch >= 2.0.0
- Gymnasium >= 0.29.0
- Flappy Bird Gymnasium >= 0.3.0
- TensorBoard >= 2.14.0
- NumPy >= 1.24.0

---

## ğŸ® Uso

### Entrenar un Nuevo Modelo

Para entrenar un agente desde cero:

```bash
python train_vector_improved.py
```

**ConfiguraciÃ³n de entrenamiento:**
- **Entornos paralelos:** 8
- **Pasos por iteraciÃ³n:** 2048
- **Ã‰pocas por actualizaciÃ³n:** 4
- **Learning rate inicial:** 3e-4
- **Clip epsilon:** 0.2
- **EntropÃ­a adaptativa:** SÃ­

El entrenamiento guardarÃ¡:
- Checkpoints en `best_model_improved.pt`
- Logs de TensorBoard en `runs/`

### Visualizar el Entrenamiento

Para monitorear el progreso en tiempo real:

```bash
tensorboard --logdir=runs
```

Luego abre tu navegador en `http://localhost:6006`

### Evaluar el Modelo Entrenado

Para ver al agente jugar:

```bash
python watch_play.py
```

**ParÃ¡metros configurables** (en el archivo):
- `episodes`: NÃºmero de episodios a visualizar (default: 5)
- `fps`: Velocidad de renderizado (default: 60)
- `debug`: Mostrar informaciÃ³n de debugging (default: True)

---

## ğŸ“Š Componentes Principales

### PPO-Clip (`ppo.py`)
ImplementaciÃ³n del algoritmo Proximal Policy Optimization con:
- Clipping de probabilidades para estabilidad
- Generalised Advantage Estimation (GAE)
- OptimizaciÃ³n de Actor y Critic simultÃ¡neos

### Red Neuronal Actor-Critic
Arquitectura de la red:
```
Input (180 observaciones) 
    â†“
Capa Compartida (256 unidades) â†’ ReLU
    â†“
Capa Compartida (256 unidades) â†’ ReLU
    â”œâ”€â†’ Policy Head (2 acciones)
    â””â”€â†’ Value Head (1 valor)
```

### NormalizaciÃ³n de Observaciones
Wrapper personalizado que mantiene estadÃ­sticas mÃ³viles (media y varianza) para normalizar observaciones durante el entrenamiento, mejorando la estabilidad del aprendizaje.

---

## ğŸ“ˆ Resultados

El modelo aprende progresivamente a:
1. **Evitar colisiones** con las tuberÃ­as
2. **Mantener altura Ã³ptima** en el juego
3. **Maximizar la recompensa** acumulada

Los mejores modelos logran superar mÃºltiples obstÃ¡culos consecutivamente.

---

## ğŸ“ Evaluaciones

Las evaluaciones completas y resultados adicionales estÃ¡n disponibles en la siguiente carpeta de Google Drive:

[Evaluaciones - Google Drive](https://drive.google.com/drive/folders/1esKJS97e0Ws1jOa9FZXQD6lc1lXqItym?usp=sharing)
---